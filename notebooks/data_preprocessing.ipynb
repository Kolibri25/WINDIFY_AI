{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rloSiKJjMe9Y"
      },
      "source": [
        "# Enviroment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCemtZtzguFF",
        "outputId": "7b65c1ae-d57f-48fe-acd4-93b72546a5a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WINDIFY_AI'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 121 (delta 43), reused 77 (delta 21), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (121/121), 92.96 KiB | 1.02 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Kolibri25/WINDIFY_AI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R2__N0AgvYx",
        "outputId": "817ad9fa-ebbc-47c8-997a-1ff9232b9707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/WINDIFY_AI\n"
          ]
        }
      ],
      "source": [
        "cd WINDIFY_AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FffSfxEO_JIN"
      },
      "source": [
        "Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kQSVikNd_Ii0",
        "outputId": "7cb63a3e-09c9-47b9-c941-f75a4392a885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4==1.7.2 (from -r requirements.txt (line 1))\n",
            "  Using cached netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 2))\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 3))\n",
            "  Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting xarray==2023.6.0 (from -r requirements.txt (line 4))\n",
            "  Using cached xarray-2023.6.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting scipy==1.10.1 (from -r requirements.txt (line 5))\n",
            "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting requests==2.28.1 (from -r requirements.txt (line 6))\n",
            "  Using cached requests-2.28.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting matplotlib==3.6.2 (from -r requirements.txt (line 7))\n",
            "  Using cached matplotlib-3.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting dask==2023.6.0 (from -r requirements.txt (line 8))\n",
            "  Using cached dask-2023.6.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting torch==1.13.0 (from -r requirements.txt (line 9))\n",
            "  Using cached torch-1.13.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.0 (from versions: 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0, 0.22.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netCDF4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0UIrfCSJ68N",
        "outputId": "89fdec1f-7ca2-40c4-f0b9-a584b89d16f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Using cached netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.6.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2.0.2)\n",
            "Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.4.post1 netCDF4-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EMv_nCdHD_u2"
      },
      "outputs": [],
      "source": [
        "import scripts.download_barra as dnb\n",
        "import scripts.load_data as ld\n",
        "from scripts.model_input import create_uv_tensor, create_uv_tensor_era5, split_tensor\n",
        "import models.unet_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OqKDxy0kDIg5"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ_7p29H-w3r"
      },
      "source": [
        "Below purely to document BARRA data download and import procedure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDEF4P5J-wNa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# download barrac2 data from months of Oct, Nov, Dec, Jan from 1979 to 2024\n",
        "# lat/lon values and variables are hard-coded in the script\n",
        "# need to check download path\n",
        "\n",
        "dnb.main()\n",
        "\n",
        "# download era5\n",
        "# TODO: write script to download era5 data\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo32avi0-5Ws"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# combine files into single dataset per variable\n",
        "# BARRA-C2\n",
        "ld.load_and_save_barra_c2()\n",
        "# ERA5\n",
        "ld.load_and_save_era5()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O-0p6e5EqGI"
      },
      "source": [
        "Importing previously downloaded data using gdown from Google Drive folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0Z95sG3gE0ca",
        "outputId": "e6d81e73-19d2-4203-9ca9-72f14e42b29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Retrieving folder contents\n",
            "Retrieving folder 1_rHNYO880eYBNB0w7ciNWKxCop2oq4Na ERA5\n",
            "Processing file 1biSKBWzXjYk8e7UWYl-QJs-bP4z0FWqe combined_era5.nc\n",
            "Retrieving folder 1f2DGtE-oKAuXlnp2re9byp4UbMitIFOf BARRA-C2\n",
            "Processing file 1MPIFRs3SoilpeLTMn7P21NyiRAOyZNIB uas_gesamt.nc\n",
            "Processing file 1hkWS9IlTFCIS0BGv0Yl0WbgkILRsaPa_ vas_gesamt.nc\n",
            "Retrieving folder 1L-QsfJgqixDRWQHN6jxEkWdid0-YjZtX processed_data\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1biSKBWzXjYk8e7UWYl-QJs-bP4z0FWqe\n",
            "To: /content/WINDIFY_AI/metfut_data/ERA5/combined_era5.nc\n",
            "100% 70.3M/70.3M [00:00<00:00, 182MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1MPIFRs3SoilpeLTMn7P21NyiRAOyZNIB\n",
            "From (redirected): https://drive.google.com/uc?id=1MPIFRs3SoilpeLTMn7P21NyiRAOyZNIB&confirm=t&uuid=926a494b-c93d-45bf-b6ef-2f4ea4cf2a75\n",
            "To: /content/WINDIFY_AI/metfut_data/BARRA-C2/uas_gesamt.nc\n",
            "100% 1.72G/1.72G [00:17<00:00, 98.7MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1hkWS9IlTFCIS0BGv0Yl0WbgkILRsaPa_\n",
            "From (redirected): https://drive.google.com/uc?id=1hkWS9IlTFCIS0BGv0Yl0WbgkILRsaPa_&confirm=t&uuid=23c3101a-76e7-468b-a24d-47c296b7fad4\n",
            "To: /content/WINDIFY_AI/metfut_data/BARRA-C2/vas_gesamt.nc\n",
            "100% 1.56G/1.56G [00:13<00:00, 115MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!gdown --folder https://drive.google.com/drive/folders/1xwqJ8Ku_US5Q7MRvExKC26TqCUroQ6QE?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHB-fSBwv4Q5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-lKr0vQ6v1VL"
      },
      "outputs": [],
      "source": [
        "!mv metfut_data/* /content/WINDIFY_AI/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QEpkVf5yr2Ty"
      },
      "outputs": [],
      "source": [
        "# load target data (BARRA-C2)\n",
        "\n",
        "uas_ds = xr.open_dataset('/content/WINDIFY_AI/data/BARRA-C2/uas_gesamt.nc')\n",
        "vas_ds = xr.open_dataset('/content/WINDIFY_AI/data/BARRA-C2/vas_gesamt.nc')\n",
        "\n",
        "# load input data (ERA5)\n",
        "\n",
        "era5_ds = xr.open_dataset('/content/WINDIFY_AI/data/ERA5/combined_era5.nc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSeJUldbMjQe"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq-QjBe-m133"
      },
      "source": [
        "Steps for Target ie. BARRA-C2:\n",
        "\n",
        "1. Drop grid points to match model dimensions (divisible by 16).\n",
        "2. Normalise the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMM0GGSH1D7f"
      },
      "source": [
        "Steps for Input ie. ERA5\n",
        "\n",
        "1.   Drop grid points to match model dimensions (divisible by 16).\n",
        "2.   Regrid EAR5 to a high-resolution grid of BARRA-C2\n",
        "3.  Normalise the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhM8QYTj3QEY"
      },
      "source": [
        "Finally, we stack the input/target together as PyTorch tensors and split the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH0x6Yufrqx2"
      },
      "source": [
        "## Target: BARRA-C2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y6BpIEUdrvi_"
      },
      "outputs": [],
      "source": [
        "# crop to make dimensions divisible by 16\n",
        "\n",
        "from scripts.crop_to_div16 import crop_to_multiple_of_16\n",
        "\n",
        "uas_ds = crop_to_multiple_of_16(uas_ds, lat_name='lat', lon_name='lon')\n",
        "vas_ds = crop_to_multiple_of_16(vas_ds, lat_name='lat', lon_name='lon')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_gwRuR1-3oRZ"
      },
      "outputs": [],
      "source": [
        "# normalise the target data\n",
        "\n",
        "from scripts.normalize_data import normalize_data\n",
        "\n",
        "uas_ds, uas_mean, uas_std = normalize_data(uas_ds,\"uas\")\n",
        "vas_ds, vas_mean, vas_std = normalize_data(vas_ds,\"vas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owb5KRxM5Fq7"
      },
      "source": [
        "## Input: ERA5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GTquY7ZZ5I9p"
      },
      "outputs": [],
      "source": [
        "# regrid to barra grid\n",
        "\n",
        "era5_interp = era5_ds.interp(\n",
        "    latitude=uas_ds.lat,\n",
        "    longitude=uas_ds.lon,\n",
        "    method='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rWMs_VyO5VSd"
      },
      "outputs": [],
      "source": [
        "# normalise input data\n",
        "\n",
        "u10_ds, u10_mean, u10_std = normalize_data(era5_interp,\"u10\")\n",
        "v10_ds, v10_mean, v10_std = normalize_data(era5_interp,\"v10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dVEA0m8y7EQ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7e93a7db-2f73-4c31-a44e-7afdf1285359"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'uas_ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-399440758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of uas_ds:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muas_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of vas_ds:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvas_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uas_ds longitude values:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muas_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'uas_ds' is not defined"
          ]
        }
      ],
      "source": [
        "# convert to PyTorch tensors\n",
        "\n",
        "print(\"Shape of uas_ds:\", uas_ds.lon.shape)\n",
        "print(\"Shape of vas_ds:\", vas_ds.lon.shape)\n",
        "print(\"uas_ds longitude values:\", uas_ds.lon.values)\n",
        "print(\"vas_ds longitude values:\", vas_ds.lon.values)\n",
        "\n",
        "# Interpolate vas_ds to match uas_ds longitude coordinates\n",
        "vas_ds_interp = vas_ds.interp(lon=uas_ds.lon, method='nearest')\n",
        "\n",
        "target = create_uv_tensor(uas_ds,vas_ds_interp)\n",
        "input = create_uv_tensor_era5(u10_ds[\"u10\"],v10_ds[\"v10\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OwrpaCT9joE"
      },
      "outputs": [],
      "source": [
        "# check shapes\n",
        "\n",
        "print(target.shape)\n",
        "print(input.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnBb_hlM9BMW"
      },
      "outputs": [],
      "source": [
        "# save in case of runtime shortage\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/metfut_data/processed_data/\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Ensure trailing slash and directory exists\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Use os.path.join to avoid hardcoding slashes\n",
        "torch.save(target, os.path.join(output_path, 'target.pt'))\n",
        "torch.save(input, os.path.join(output_path, 'input.pt'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vzXbYPR9UHn"
      },
      "outputs": [],
      "source": [
        "# train/validation/test split\n",
        "\n",
        "x_train, x_val, x_test = split_tensor(input)\n",
        "y_train, y_val, y_test = split_tensor(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HffKdy7Y63S1"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}